<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Part 2 – When Numbers Start to Mean Something</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- MathJax for TeX-style equations -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']]
      }
    };
  </script>
  <script async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Theme:
       #f6eaaf (light yellow), #ead3c1 (soft peach), #941236 (deep maroon) -->
  <style>
    :root {
      --bg-main: #f6eaaf;
      --bg-card: #ffffff;
      --bg-soft: #ead3c1;
      --accent:  #941236;
      --text-main: #222222;
      --text-muted: #555555;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      padding: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background:
        linear-gradient(135deg, rgba(234, 211, 193, 0.4), rgba(246, 234, 175, 0.6)),
        var(--bg-main);
      color: var(--text-main);
      line-height: 1.75;
      font-size: 18px;
    }

    .page-wrapper {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2.5rem 1.25rem 3.5rem;
    }

    .blog-card {
      max-width: 1100px;
      margin: 0 auto;
      background: var(--bg-card);
      border-radius: 1.25rem;
      box-shadow:
        0 16px 40px rgba(0, 0, 0, 0.08),
        0 -6px 18px rgba(148, 18, 54, 0.06);
      padding: 2.5rem 2.75rem;
      border-top: 6px solid var(--accent);
    }

    @media (max-width: 768px) {
      .blog-card {
        padding: 1.75rem 1.5rem;
        border-radius: 1rem;
      }
    }

    .blog-meta {
      font-size: 0.9rem;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--text-muted);
      margin-bottom: 0.75rem;
    }

    .series-tag {
      display: inline-flex;
      align-items: center;
      gap: 0.4rem;
      padding: 0.25rem 0.7rem;
      border-radius: 999px;
      background: var(--bg-soft);
      color: var(--accent);
      font-size: 0.75rem;
      font-weight: 600;
      letter-spacing: 0.12em;
      text-transform: uppercase;
    }

    .series-tag-dot {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: var(--accent);
    }

    h1 {
      font-size: 2.4rem;
      margin: 0.4rem 0 0.75rem;
      color: var(--accent);
      letter-spacing: 0.02em;
    }

    .blog-subtitle {
      font-size: 1.05rem;
      color: var(--text-muted);
      margin-bottom: 1.8rem;
    }

    h2 {
      font-size: 1.6rem;
      margin-top: 2.25rem;
      margin-bottom: 0.6rem;
      color: var(--accent);
    }

    h3 {
      font-size: 1.2rem;
      margin-top: 1.6rem;
      margin-bottom: 0.4rem;
      color: #7a0f29;
    }

    p {
      margin: 0.5rem 0 0.9rem;
      font-size: 1rem;
    }

    strong {
      color: #3a111c;
    }

    .emphasis-line {
      padding: 0.7rem 1rem;
      border-left: 4px solid var(--accent);
      background: #fff8e7;
      border-radius: 0.6rem;
      font-size: 0.98rem;
      margin: 1rem 0 1.4rem;
    }

    .timeline-callout {
      background: var(--bg-soft);
      border-radius: 0.9rem;
      padding: 1rem 1.1rem;
      margin: 1.4rem 0;
      font-size: 0.95rem;
    }

    .timeline-callout-title {
      font-weight: 600;
      color: var(--accent);
      margin-bottom: 0.35rem;
      text-transform: uppercase;
      font-size: 0.8rem;
      letter-spacing: 0.14em;
    }

    ul {
      padding-left: 1.3rem;
      margin: 0.25rem 0 1rem;
    }

    li {
      margin: 0.2rem 0;
      font-size: 1rem;
    }

    .muted {
      color: var(--text-muted);
    }

    .inline-question {
      font-style: italic;
      color: #4a222b;
    }

    .section-divider {
      margin: 2.2rem 0 1.8rem;
      height: 1px;
      border: none;
      background: linear-gradient(to right, transparent, #d7b2a3, transparent);
    }

    /* Image placeholders */
    .image-block {
      margin: 1.6rem 0 1.3rem;
    }

    .image-frame {
      width: 100%;
      max-width: 720px;
      height: 260px;
      margin: 0 auto;
      border-radius: 0.9rem;
      border: 2px dashed rgba(148, 18, 54, 0.35);
      background: linear-gradient(
        135deg,
        rgba(246, 234, 175, 0.5),
        rgba(234, 211, 193, 0.6)
      );
      display: flex;
      align-items: center;
      justify-content: center;
      overflow: hidden;
      position: relative;
    }

    .image-frame img {
      width: 100%;
      height: 100%;
      object-fit: contain;
      display: block;
    }

    .image-placeholder-label {
      position: absolute;
      bottom: 10px;
      right: 12px;
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.14em;
      color: rgba(148, 18, 54, 0.8);
      background: rgba(255, 255, 255, 0.75);
      padding: 0.25rem 0.55rem;
      border-radius: 999px;
    }

    .image-placeholder-text {
      font-size: 0.9rem;
      color: #5c2a33;
      text-align: center;
      padding: 0 1.5rem;
    }

    .image-caption {
      max-width: 720px;
      margin: 0.5rem auto 0.2rem;
      font-size: 0.88rem;
      color: var(--text-muted);
      text-align: center;
    }

    @media (max-width: 768px) {
      .image-frame {
        height: 210px;
      }
      .image-placeholder-text {
        font-size: 0.85rem;
        padding: 0 1rem;
      }
    }
  </style>
</head>
<body>
  <div class="page-wrapper">
    <article class="blog-card">

      <div class="blog-meta">
        <span class="series-tag">
          <span class="series-tag-dot"></span>
          LLM from first principles
        </span>
      </div>

      <h1>Part&nbsp;2 &mdash; When Numbers Start to Mean Something</h1>
      <p class="blog-subtitle">
        From handwritten digits to the idea of a “geometry of meaning” &mdash; and why we want something similar for text.
      </p>

      <!-- SECTION 1 -->
      <h2>1. From “Text as Numbers” to “Numbers With Meaning”</h2>

      <p>
        In the previous post, we left computers in an awkward place. They had:
      </p>
      <ul>
        <li><strong>ASCII</strong> and <strong>Unicode</strong> to map characters to integers.</li>
        <li><strong>UTF-8</strong> to store and transmit those integers.</li>
      </ul>

      <p>
        But these integers had <em>no structure</em>. Nothing about “65” versus “97” tells you anything deep about
        <code>'A'</code> versus <code>'a'</code>. They’re just IDs in a very official table.
      </p>

      <p class="emphasis-line">
        If we want models that behave as if they understand language, we need more than IDs.  
        We need numbers that carry <strong>meaningful relationships</strong>.
      </p>

      <p>
        This sounds abstract. So before we touch language again, let’s visit a place where it is much easier to see:
        <strong>images</strong>. In particular, the tiny, iconic image dataset called <strong>MNIST</strong>.
      </p>

      <hr class="section-divider">

      <!-- SECTION 2 -->
      <h2>2. MNIST: 70,000 Handwritten “Hello World”s</h2>

      <p>
        MNIST is one of the simplest and most famous datasets in machine learning. If ML had a kindergarten, MNIST would be
        the first activity sheet.
      </p>

      <p>It consists of:</p>
      <ul>
        <li>70,000 grayscale images of handwritten digits (0–9),</li>
        <li>each image is <strong>28 × 28 pixels</strong>,</li>
        <li>each pixel stores a brightness value (e.g., from 0 to 255).</li>
      </ul>

      <p>
        Some digits look like they came from a math textbook. Others look like the writer sneezed halfway through a “5”.
        But as humans, we can still easily say: “Yes, that’s a 5. Yes, that’s a 9. That one… okay, maybe a stressed-out 2.”
      </p>

      <!-- IMAGE PLACEHOLDER 1: MNIST GRID -->
      <div class="image-block">
        <div class="image-frame">
          <!-- Replace the placeholder below with your real image, e.g.:
               <img src="assets/llm-series/part2-mnist-grid.png" alt="Grid of MNIST handwritten digit samples"> -->
          <div class="image-placeholder-text">
            <strong>Image placeholder:</strong> a 6×6 or 8×8 grid of MNIST digit samples  
            (0–9 written by different people).
          </div>
          <div class="image-placeholder-label">MNIST overview</div>
        </div>
        <p class="image-caption">
          Suggested visual: a small grid of MNIST digit images showing the diversity of handwritten 0–9.
        </p>
      </div>

      <p>
        Now here’s the key ML perspective: the computer does not see “pictures of numbers”. It sees a list of pixel intensities.
      </p>

      <p>
        Each digit becomes a vector with <strong>784 numbers</strong> (because 28 × 28 = 784). So mathematically, we can think of each image as:
      </p>

      <p class="emphasis-line">
        \[
          \text{image} \;\longrightarrow\; (x_1, x_2, \dots, x_{784}) \in \mathbb{R}^{784}.
        \]
      </p>

      <p>
        MNIST, from the model’s point of view, is not “a collection of cute little pictures”, it is:
      </p>

      <p class="inline-question">
        70,000 points floating around in a 784-dimensional universe.
      </p>

      <hr class="section-divider">

      <!-- SECTION 3 -->
      <h2>3. Distance: When Geometry Starts Whispering “These Two Are Alike”</h2>

      <p>
        Take two images of the digit <strong>3</strong> from MNIST, written by two different people. As humans:
      </p>
      <ul>
        <li>we know both are “3”,</li>
        <li>we can see they’re a bit different in style, but the identity is clear.</li>
      </ul>

      <p>
        As vectors, they’re just two points in \( \mathbb{R}^{784} \). We can ask a purely geometric question:
      </p>

      <p class="emphasis-line">
        <strong>How far apart are these two points?</strong>
      </p>

      <p>
        The standard answer is the <strong>Euclidean distance</strong> (the usual distance formula, extended to many dimensions):
      </p>

      <p>
        \[
          d(\mathbf{x}, \mathbf{y}) \;=\; \sqrt{\sum_{i=1}^{784} (x_i - y_i)^2}.
        \]
      </p>

      <p>
        Intuition:
      </p>
      <ul>
        <li>If two images look similar → most of their pixel values are similar → the distance is <strong>small</strong>.</li>
        <li>If one is clearly “3” and the other clearly “9” → many pixel values differ → the distance is <strong>larger</strong>.</li>
      </ul>

      <!-- IMAGE PLACEHOLDER 2: POINTS IN SPACE / CLUSTERS -->
      <div class="image-block">
        <div class="image-frame">
          <!-- Replace with real visual later, e.g. t-SNE or UMAP 2D projection of MNIST -->
          <!-- <img src="assets/llm-series/part2-mnist-clusters.png" alt="2D projection of MNIST images with clusters by digit"> -->
          <div class="image-placeholder-text">
            <strong>Image placeholder:</strong> a 2D scatter plot (t-SNE / UMAP style) where each point is an MNIST image,  
            coloured by digit label (0–9), showing clusters forming in space.
          </div>
          <div class="image-placeholder-label">Clusters emerge</div>
        </div>
        <p class="image-caption">
          Suggested visual: a 2D projection where “3”s cluster near “3”s, “8”s near “8”s, etc. Geometry begins to reflect concept.
        </p>
      </div>

      <p>
        Now imagine: for a given “3”, you search for its <strong>nearest neighbours</strong> in this high-dimensional space.
      </p>
      <ul>
        <li>Most of the closest points will be other “3”s.</li>
        <li>Occasionally you might find a confused “5” that looks like a “3”.</li>
      </ul>

      <p class="emphasis-line">
        Without telling the computer what a “3” <em>means</em>, distance has already created a crude sense of “these belong together”.
      </p>

      <p>
        Geometry is doing a surprising amount of semantic work for us:
        clusters and neighbourhoods start lining up with our human sense of “same digit”.
      </p>

      <hr class="section-divider">

      <!-- SECTION 4 -->
      <h2>4. Cosine Similarity: When Direction Beats Raw Distance</h2>

      <p>
        Euclidean distance is one way to compare image vectors. Another popular trick is 
        <strong>cosine similarity</strong>.
      </p>

      <p>
        Think of every image vector as an arrow from the origin in 784D space. Two arrows can:
      </p>
      <ul>
        <li>point in almost the same direction, or</li>
        <li>point in very different directions.</li>
      </ul>

      <p>
        Cosine similarity measures the <strong>angle</strong> between these two arrows. Formally:
      </p>

      <p>
        \[
          \cos(\theta) \;=\; \frac{\mathbf{x} \cdot \mathbf{y}}{\lVert \mathbf{x} \rVert \, \lVert \mathbf{y} \rVert}.
        \]
      </p>

      <p>
        Why is this useful?
      </p>
      <ul>
        <li>If two images differ only in overall brightness (one is slightly darker), their vectors might have different lengths, but point in a similar direction → cosine similarity is high.</li>
        <li>If they are completely different digits, the vectors’ directions are more different → cosine similarity is lower.</li>
      </ul>

      <!-- IMAGE PLACEHOLDER 3: DISTANCE VS COSINE -->
      <div class="image-block">
        <div class="image-frame">
          <!-- Replace with a conceptual diagram later -->
          <!-- <img src="assets/llm-series/part2-distance-vs-cosine.png" alt="Comparison of Euclidean distance and cosine similarity between two vectors"> -->
          <div class="image-placeholder-text">
            <strong>Image placeholder:</strong> a simple 2D diagram showing two vectors:  
            (1) Euclidean distance as the straight-line gap,  
            (2) cosine similarity as the angle between them.
          </div>
          <div class="image-placeholder-label">Distance vs angle</div>
        </div>
        <p class="image-caption">
          Suggested visual: two vectors from the origin, annotated with “Euclidean distance” (length of the gap) and “cosine similarity” (angle).
        </p>
      </div>

      <p class="emphasis-line">
        With cosine similarity, we care more about the <strong>shape of the pattern</strong> of pixel intensities and less about global brightness.
      </p>

      <p>
        Again, the outcome is similar: “3”s tend to have high cosine similarity with other “3”s, and lower similarity with “7”s or “9”s.
      </p>

      <p>
        We still haven’t said a word about counting, arithmetic, or what the symbol “3” stands for in the real world. All we’ve done is:
      </p>
      <ul>
        <li>represent images as vectors,</li>
        <li>define a way to measure similarity between those vectors.</li>
      </ul>

      <p>
        And yet, geometry is starting to line up with meaning.
      </p>

      <hr class="section-divider">

      <!-- SECTION 5 -->
      <h2>5. Meaning as Geometry</h2>

      <p>
        If we zoom out, the pattern is this:
      </p>

      <ul>
        <li>Each MNIST digit → point in a high-dimensional numeric space.</li>
        <li>We define distance / similarity between points (Euclidean, cosine, etc.).</li>
        <li>Points naturally form <strong>clusters</strong>.</li>
      </ul>

      <p>
        Inside each cluster:
      </p>
      <ul>
        <li>the points correspond to images that humans perceive as “the same digit”,</li>
        <li>differences within a cluster are mostly stylistic variations (how someone writes a “3”).</li>
      </ul>

      <p class="emphasis-line">
        Geometry becomes a proxy for meaning: “nearby” means “similar”, “far away” means “different”.
      </p>

      <p>
        We never explicitly encoded that “3” is a number that comes after 2 and before 4.  
        All we did is:
      </p>
      <ul>
        <li>pick a numeric representation (pixels),</li>
        <li>define a notion of closeness,</li>
        <li>let the structure of the data reveal itself in that space.</li>
      </ul>

      <p>
        This is the first big philosophical step toward building intelligent systems:
        <strong>meaning emerges as geometry in a space of numbers.</strong>
      </p>

      <!-- IMAGE PLACEHOLDER 4: SIDE-BY-SIDE ANALOGY IMAGE/TEXT -->
      <div class="image-block">
        <div class="image-frame">
          <!-- Replace with your own designed visual -->
          <!-- <img src="assets/llm-series/part2-images-vs-words-space.png" alt="Analogy between image vector space and word embedding space"> -->
          <div class="image-placeholder-text">
            <strong>Image placeholder:</strong> left side: MNIST points clustered by digit.  
            Right side: conceptual “word embedding” space with words like “cat, dog, tiger” near each other,  
            and “red, blue, green” forming another cluster.
          </div>
          <div class="image-placeholder-label">Analogy: images ↔ words</div>
        </div>
        <p class="image-caption">
          Suggested visual: a side-by-side analogy between the geometry of images and the geometry of words/tokens.
        </p>
      </div>

      <hr class="section-divider">

      <!-- SECTION 6 -->
      <h2>6. Can We Do the Same for Words and Numbers?</h2>

      <p>
        Let’s now compare the situation with images to where we left text in Part&nbsp;1.
      </p>

      <h3>Images (MNIST):</h3>
      <ul>
        <li>Each image → a vector in \( \mathbb{R}^{784} \).</li>
        <li>We define distance / cosine similarity.</li>
        <li>Visually similar images → vectors that live near each other.</li>
        <li>Clusters naturally correspond to “this is a 3”, “this is an 8”, etc.</li>
      </ul>

      <h3>Text (so far):</h3>
      <ul>
        <li>Each character → Unicode code point (e.g., <code>'A'</code> → 65, <code>'a'</code> → 97).</li>
        <li>Each word → sequence of code points (e.g., “cat” → [99, 97, 116]).</li>
        <li>Those numbers are <em>IDs</em>, not geometric coordinates.</li>
        <li>There is no meaningful distance between “cat” and “dog” as [99, 97, 116] vs [100, 111, 103].</li>
      </ul>

      <p class="emphasis-line">
        If we treat Unicode numbers as coordinates, the geometry is meaningless.  
        Similar words will not be close. Synonyms will not cluster. Everything will look like noise.
      </p>

      <p>
        But we now know that:
      </p>
      <ul>
        <li>for images, a good numeric representation + a distance measure can give us a geometry that reflects meaning,</li>
        <li>so it’s natural to ask if we can build a similar geometry for <strong>words</strong> or <strong>tokens</strong>.</li>
      </ul>

      <p class="inline-question">
        Can we design a space where “cat” and “dog” live near each other,  
        “king” and “queen” are related in a structured way,  
        and “walk”, “walked”, “walking” sit close together?
      </p>

      <p>
        That is exactly what we aim for in modern NLP:
      </p>
      <ul>
        <li>Choose the right <strong>units</strong> of text (this is where <strong>tokenisation</strong> comes in).</li>
        <li>Map each token to a vector in a high-dimensional space (these are <strong>embeddings</strong>).</li>
        <li>Let distances and angles in that space stand in for semantic similarity.</li>
      </ul>

      <p>
        In other words, we want to recreate for words what we already saw with MNIST digits:
      </p>

      <p class="emphasis-line">
        <strong>Tokens → vectors → geometry → meaning.</strong>
      </p>

      <hr class="section-divider">

      <!-- SECTION 7 -->
      <h2>7. Where We Are in the Series</h2>

      <p>
        Across the first two posts, we’ve slowly shifted from:
      </p>
      <ul>
        <li>computers that can only crunch numbers,</li>
        <li>to character encodings (ASCII, Unicode) that represent text as IDs,</li>
        <li>to image vectors and similarity measures where geometry starts to reflect meaning.</li>
      </ul>

      <p>
        This post’s main goal was to show that:
      </p>
      <p class="emphasis-line">
        Once you have a good numeric representation and a sensible notion of distance,  
        <strong>meaning can start to appear as structure in a vector space</strong>.
      </p>

      <p>
        The next step in this series is to apply that idea directly to language:
      </p>
      <ul>
        <li>How do we decide which text units to turn into tokens?</li>
        <li>What is wrong with the “one token per word” idea?</li>
        <li>How do embeddings turn discrete token IDs into learnable vectors?</li>
        <li>And how does all of this set the stage for transformers and LLMs?</li>
      </ul>

      <p class="muted">
        In other words: we’ve seen how geometry can give images meaning.  
        Next, we’ll start constructing a geometry where language itself can live.
      </p>

    </article>
  </div>
</body>
</html>
